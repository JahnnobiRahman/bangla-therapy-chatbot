import streamlit as st
import requests

# Query function for LM Studio (chat-completion)
def query_llm(prompt):
    try:
        response = requests.post(
            "http://192.168.0.125:1234/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": "mistral-7b-instruct-v0.3",
                "messages": [
                    {
                        "role": "user",
                        "content": f"""ржЖржкржирж┐ ржПржХржЬржи рж╕рж╣рж╛ржирзБржнрзВрждрж┐рж╢рзАрж▓ ржмрж╛ржВрж▓рж╛ ржерзЗрж░рж╛ржкрж┐рж╕рзНржЯред
ржЖржорж╛рж░ ржкрзНрж░рж╢рзНржи: {prompt}"""
                    }
                ],
                "temperature": 0.7
            }
        )

        if response.status_code != 200:
            return f"тЪая╕П рж╕рж╛рж░рзНржнрж╛рж░ рждрзНрж░рзБржЯрж┐ (status code: {response.status_code})\n\n{response.text}"

        data = response.json()

        if "choices" not in data:
            return f"тЪая╕П 'choices' ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯржирж┐:\n\n{data}"

        return data["choices"][0]["message"]["content"]

    except Exception as e:
        return f"тЭМ рждрзНрж░рзБржЯрж┐ ржШржЯрзЗржЫрзЗ:\n\n{e}"

# Streamlit UI
st.title("ЁЯза ржмрж╛ржВрж▓рж╛ ржерзЗрж░рж╛ржкрж┐ ржЪрзНржпрж╛ржЯржмржЯ ЁЯдЦЁЯТм")

user_input = st.text_input("ржЖржкржирж╛рж░ ржХржерж╛ рж▓рж┐ржЦрзБржи", key="user_input_main")

if user_input:
    response = query_llm(user_input)
    st.write("ЁЯза ржерзЗрж░рж╛ржкрж┐рж╕рзНржЯ: " + response)
