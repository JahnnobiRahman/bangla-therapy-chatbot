import streamlit as st
import requests
from datetime import datetime
import time
import os
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema #added output parser and response schema


# Page setup
st.set_page_config(page_title="‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü", page_icon="üß†", layout="wide")

# Sidebar setup
with st.sidebar:
    st.title("‚öôÔ∏è Settings")

    # Show current working directory
    cwd = os.getcwd()
    
    st.markdown("üìÅ **Current Working Directory:**")
    st.code(cwd)

    file_path = os.path.join(cwd, "/Users/jahnnobirahman/Desktop/python/allcode/projects/l01/relaxy-rag/therapy_knowledge.txt")
    st.markdown("üìÑ **Trying to load file from:**")
    st.code(file_path)

    # Load the file and preview
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            docs = f.readlines()
            st.session_state.documents = docs
            st.success("‚úÖ File loaded successfully!")
            st.markdown("üìÑ Preview of file:")
            st.code("".join(docs[:5]), language="text")
    except Exception as e:
        st.error(f"‚ùå File loading failed:\n{e}")

    # Server status check
    try:
        requests.get("http://192.168.0.104:1235/v1/models", timeout=5)
        st.success("‚úÖ ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡¶æ‡¶®‡ßá‡¶ï‡ßç‡¶ü‡ßá‡¶°")
    except:
        st.error("‚ùå ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶®‡¶æ‡¶á")

    # Clear history button
    if st.button("üóëÔ∏è Clear chat history"):
        st.session_state.messages = []
        st.rerun()

# Session state initialization
if "messages" not in st.session_state:
    st.session_state.messages = []

if "documents" not in st.session_state:
    st.session_state.documents = []

# Context retrieval function
def retrieve_context(user_prompt):
    keywords = user_prompt.lower().split()
    relevant = [doc for doc in st.session_state.documents if any(word in doc.lower() for word in keywords)]
    return "\n".join(relevant[:3])


#  Structured output schema
response_schemas = [
    ResponseSchema(name="problem", description="‡¶á‡¶â‡¶ú‡¶æ‡¶∞‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ"),
    ResponseSchema(name="suggestion", description="‡¶∏‡¶æ‡¶á‡¶ï‡ßã‡¶≤‡¶ú‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂")
]
parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = parser.get_format_instructions()

#  Query the local LLM (Qwen 1.5 - 1.8B) with context
def query_llm(user_prompt):
    try:
        context = retrieve_context(user_prompt)
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶π‡¶æ‡¶®‡ßÅ‡¶≠‡ßÇ‡¶§‡¶ø‡¶∂‡ßÄ‡¶≤ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø‡¶∏‡ßç‡¶ü‡•§ ‡¶®‡¶ø‡¶ö‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶ï‡¶®‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¶‡ßá‡ßü‡¶æ ‡¶π‡¶≤‡ßã:\n{context}"),
            ("user", "{question}")
        ])
        final_prompt = prompt_template.format(context=context, question=user_prompt)

        response = requests.post(
            "http://192.168.0.104:1235/v1/chat/completions",
            headers={"Content-Type": "application/json"},
            json={
                "model": "Qwen1.5-1.8B-Chat-GGUF",
                "messages": [{"role": "user", "content": final_prompt}],
                "temperature": 0.7,
                "max_tokens": 500
            },
            timeout=20
        )

        if response.status_code == 200:
            data = response.json()
            return data["choices"][0]["message"]["content"]
        else:
            return f"‚ùå API ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø: {response.status_code} - {response.text}"

    except requests.Timeout:
        return "‚ùå ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶∂‡ßá‡¶∑ ‡¶≠‡¶æ‡¶á‡ßü‡¶æ‡•§"
    except requests.ConnectionError:
        return "‚ùå ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞‡ßá ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó ‡¶™‡¶æ‡¶á‡¶®‡¶æ‡•§ LM Studio ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶¶‡ßç‡¶Ø‡¶æ‡¶ñ?‡•§"
    except Exception as e:
        return f"‚ùå ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø:\n{e}"

#  Main content
st.title("üß† ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü ü§ñüí¨")

# Show chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        st.caption(message["timestamp"])

# Chat input from user
if prompt := st.chat_input("‚úçÔ∏è ‡¶ï‡¶ø ‡¶≠‡¶æ‡¶¨‡ßá‡¶®? ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡ßá‡¶®..."):
    # Save user message
    st.session_state.messages.append({
        "role": "user",
        "content": prompt,
        "timestamp": datetime.now().strftime("%H:%M")
    })

    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
        st.caption(datetime.now().strftime("%H:%M"))

    # Get assistant response
    with st.chat_message("assistant"):
        with st.spinner("‚è≥ ‡¶≠‡¶æ‡¶¨‡¶õ‡¶ø..."):
            start_time = time.time()
            response = query_llm(prompt)
            end_time = time.time()

            st.markdown(response)
            st.caption(f"{datetime.now().strftime('%H:%M')} ‚Ä¢ {end_time - start_time:.1f} ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°")

            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "timestamp": datetime.now().strftime("%H:%M")
            })

#  Footer
st.markdown("---")
st.markdown("Made with ‚ù§Ô∏è using Streamlit + Qwen1.5-1.8B-Chat-GGUF + LangChain")
